{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Splite Your Data to Three : CSV and JSON and DB_file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add you persentage data :\n",
    "# p_json : for json \n",
    "# p-Db : for Base de donnee\n",
    "# csv : is the keep pesontage .\n",
    "\n",
    "def data_spliter(p_json,p_DB,csv_file_name,file_json_name_part,driver,server,database,table_name,file_Csv_name_part):\n",
    "\n",
    "    # parameter cheking :\n",
    "    if p_json + p_DB <= 100 and '.csv' in csv_file_name :\n",
    "        # read csv :\n",
    "        df = pd.read_csv(csv_file_name)\n",
    "\n",
    "        # drop the NuLL value :\n",
    "        df.dropna(axis=1, inplace=True)\n",
    "\n",
    "        # do random for row in DataFrame :\n",
    "        df_random_order = df.sample(frac=1, random_state=42)\n",
    "\n",
    "        # list as names of all columns :\n",
    "        columns = list(df_random_order.columns)\n",
    "\n",
    "        # calculation of row in such type csv json db :\n",
    "        p_json = int(df_random_order.shape[0] * (p_json  * 0.01))\n",
    "        p_DB = int(df_random_order.shape[0] * (p_DB  * 0.01))\n",
    "        p_csv = df_random_order.shape[0] - (p_json + p_DB)\n",
    "        #----------------------------------------------------------------\n",
    "        # creating file json :\n",
    "        # --------------------\n",
    "        # all row for json file :\n",
    "        json_data_list = df_random_order[0:p_json].to_dict(orient='records')\n",
    "\n",
    "        # create file json :\n",
    "        with open(f'{file_json_name_part}_{p_json}_row.json', 'w') as json_file :\n",
    "            json.dump(json_data_list, json_file, indent=4)\n",
    "\n",
    "        #----------------------------------------------------------------\n",
    "        # creating Data Base :\n",
    "        #---------------------\n",
    "            # part of data for Db :\n",
    "            db_data = df_random_order[p_json + 1:p_json + 1 + p_DB]\n",
    "\n",
    "            # CrÃ©er un connexion dans le database :\n",
    "            driver = 'SQL Server'\n",
    "\n",
    "            conn = pyodbc.connect(f'''\n",
    "                                    DRIVER={driver};\n",
    "                                    SERVER={server};\n",
    "                                    DATABASE={database};\n",
    "                                    Trusted_Connection=yes\n",
    "                                ''')\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # creating Table :\n",
    "            query = f\"CREATE TABLE [{table_name}_{p_DB}_row] (\\n\"\n",
    "\n",
    "            for col in columns:\n",
    "                query += f\"    {col} VARCHAR(MAX),\\n\"\n",
    "\n",
    "            query = query[:-2] + \"\\n);\"\n",
    "\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "\n",
    "            # Create the SQL INSERT query template\n",
    "            insert_query = f\"INSERT INTO [{table_name}_{p_DB}_row] ({', '.join(columns)}) VALUES ({', '.join(['?'] * len(columns))})\"\n",
    "\n",
    "            # Iterate through the DataFrame using iterrows() and execute the INSERT queries\n",
    "            for _, row in db_data.iterrows():\n",
    "                var = []\n",
    "                for i in columns :\n",
    "                    var.append(row[i])\n",
    "                cursor.execute(insert_query, var)\n",
    "            cursor.commit()\n",
    "        #----------------------------------------------------------------\n",
    "        # create csv file :\n",
    "        #---------------------\n",
    "        # part of data comme csv :\n",
    "        if p_json + p_DB != 100 :\n",
    "            Csv_data = df_random_order[p_json + 1 + p_DB + 1:]\n",
    "\n",
    "            # create file csv :\n",
    "            df.to_csv(f'{file_Csv_name_part}_{Csv_data.shape[0]}_row.csv', index=False, header=False)\n",
    "        #----------------------------------------------------------------\n",
    "    else :\n",
    "        print('param error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spliter(58,15,'books.csv','lovelove','my_jsonfile','LAPTOP-6D8J0VI4\\SQLEXPRESS','xxxx','loooooo','my_csv_tes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
