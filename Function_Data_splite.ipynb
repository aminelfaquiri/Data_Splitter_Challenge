{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction pour Diviser Vos Données en Trois : CSV, JSON et Base de Données :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction divise un CSV en trois parties : \n",
    "- une pourcentage pour JSON,\n",
    "- une pourcentage pour la base de données,\n",
    "- et le reste pour le CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirement : \n",
    "- SQL SERVER .\n",
    "- database alrady create .\n",
    "- Dataset with format csv .\n",
    "- Data already cleaning .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_json              : Pourcentage de la partie JSON.\n",
    "# p_Db                : Pourcentage de la partie base de données.\n",
    "# csv_file_name       : Nom de votre fichier d'origine au format CSV.\n",
    "# file_json_name_part : Nom du fichier JSON.\n",
    "# server              : Nom de votre serveur SQL.\n",
    "# database            : Nom de votre base de données.\n",
    "# table_name          : Nom de votre table.\n",
    "# file_Csv_name_part  : Nom du fichier CSV résultant.\"\n",
    "\n",
    "def data_spliter(p_json, p_DB, csv_file_name, file_json_name_part, server, database, table_name, file_Csv_name_part):\n",
    "    # Vérification des paramètres :\n",
    "    if p_json + p_DB <= 100 and '.csv' in csv_file_name:\n",
    "        # Lecture du fichier CSV :\n",
    "        df = pd.read_csv(csv_file_name)\n",
    "\n",
    "        # Suppression des valeurs NULL :\n",
    "        df.dropna(axis=1, inplace=True)\n",
    "\n",
    "        # Mélange aléatoire des lignes du DataFrame :\n",
    "        df_random_order = df.sample(frac=1, random_state=42)\n",
    "\n",
    "        # Liste des noms de toutes les colonnes :\n",
    "        columns = list(df_random_order.columns)\n",
    "\n",
    "        # Calcul du nombre de lignes pour chaque type (csv, json, db) :\n",
    "        p_json = int(df_random_order.shape[0] * (p_json * 0.01))\n",
    "        p_DB = int(df_random_order.shape[0] * (p_DB * 0.01))\n",
    "        p_csv = df_random_order.shape[0] - (p_json + p_DB)\n",
    "\n",
    "        # Création du fichier JSON :\n",
    "        json_data_list = df_random_order[0:p_json].to_dict(orient='records')\n",
    "        with open(f'{file_json_name_part}_{p_json}_row.json', 'w') as json_file:\n",
    "            json.dump(json_data_list, json_file, indent=4)\n",
    "\n",
    "        # Création de la Base de Données :\n",
    "        db_data = df_random_order[p_json + 1:p_json + 1 + p_DB]\n",
    "        driver = 'SQL Server'\n",
    "        conn = pyodbc.connect(f'''\n",
    "                                DRIVER={driver};\n",
    "                                SERVER={server};\n",
    "                                DATABASE={database};\n",
    "                                Trusted_Connection=yes\n",
    "                            ''')\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Création de la table :\n",
    "        query = f\"CREATE TABLE [{table_name}_{p_DB}_row] (\\n\"\n",
    "        for col in columns:\n",
    "            query += f\"    {col} VARCHAR(MAX),\\n\"\n",
    "        query = query[:-2] + \"\\n);\"\n",
    "        cursor.execute(query)\n",
    "        cursor.commit()\n",
    "\n",
    "        # Création et exécution des requêtes d'INSERT pour la base de données :\n",
    "        insert_query = f\"INSERT INTO [{table_name}_{p_DB}_row] ({', '.join(columns)}) VALUES ({', '.join(['?'] * len(columns))})\"\n",
    "        for _, row in db_data.iterrows():\n",
    "            var = []\n",
    "            for i in columns:\n",
    "                var.append(row[i])\n",
    "            cursor.execute(insert_query, var)\n",
    "        cursor.commit()\n",
    "\n",
    "        # Création du fichier CSV :\n",
    "        if p_json + p_DB != 100:\n",
    "            Csv_data = df_random_order[p_json + 1 + p_DB + 1:]\n",
    "            df.to_csv(f'{file_Csv_name_part}_{Csv_data.shape[0]}_row.csv', index=False, header=False)\n",
    "\n",
    "    else:\n",
    "        print('Erreur de paramètres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spliter(58,15,'books.csv','mybook','LAPTOP-6D8J0VI4\\SQLEXPRESS','book_db','book_table','my_csv_part')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
